/// Generated by rustemo. Do not edit manually!
use std::fmt::Debug;
use std::hash::{Hash, Hasher};
use rustemo::Result;
use rustemo::lexer::{self, Token};
use rustemo::parser::Parser;
use rustemo::builder::Builder;
use rustemo::lr::builder::LRBuilder;
use regex::Regex;
use once_cell::sync::Lazy;
use rustemo::lexer::StringLexer;
pub type Input = str;
use super::output_dir_actions;
use rustemo::lr::parser::{ParserDefinition, LRParser};
use rustemo::lr::parser::Action::{self, Shift, Reduce, Accept, Error};
#[allow(unused_imports)]
use rustemo::debug::{log, logn};
#[allow(unused_imports)]
#[cfg(debug_assertions)]
use colored::*;
const TERMINAL_COUNT: usize = 3usize;
const NONTERMINAL_COUNT: usize = 5usize;
const STATE_COUNT: usize = 7usize;
#[allow(dead_code)]
const MAX_ACTIONS: usize = 2usize;
const MAX_RECOGNIZERS: usize = 2usize;
pub type Context<'i> = lexer::Context<'i, Input>;
#[allow(clippy::upper_case_acronyms)]
#[derive(Debug, Default, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash)]
pub enum TokenKind {
    #[default]
    STOP,
    Tb,
    Num,
}
#[allow(clippy::enum_variant_names)]
#[derive(Clone, Copy)]
pub enum ProdKind {
    AP1,
    B1P1,
    B1P2,
    BP1,
}
impl std::fmt::Debug for ProdKind {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let name = match self {
            ProdKind::AP1 => "A: B1 Num",
            ProdKind::B1P1 => "B1: B1 B",
            ProdKind::B1P2 => "B1: B",
            ProdKind::BP1 => "B: Tb",
        };
        write!(f, "{}", name)
    }
}
#[allow(clippy::upper_case_acronyms)]
#[allow(dead_code)]
#[derive(Clone, Copy, Debug)]
pub enum NonTermKind {
    EMPTY,
    AUG,
    A,
    B1,
    B,
}
impl From<ProdKind> for NonTermKind {
    fn from(prod: ProdKind) -> Self {
        match prod {
            ProdKind::AP1 => NonTermKind::A,
            ProdKind::B1P1 => NonTermKind::B1,
            ProdKind::B1P2 => NonTermKind::B1,
            ProdKind::BP1 => NonTermKind::B,
        }
    }
}
#[allow(clippy::enum_variant_names)]
#[derive(Default, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]
pub enum State {
    #[default]
    AUGS0,
    TbS1,
    AS2,
    B1S3,
    BS4,
    NumS5,
    BS6,
}
impl std::fmt::Debug for State {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let name = match self {
            State::AUGS0 => "0:AUG",
            State::TbS1 => "1:Tb",
            State::AS2 => "2:A",
            State::B1S3 => "3:B1",
            State::BS4 => "4:B",
            State::NumS5 => "5:Num",
            State::BS6 => "6:B",
        };
        write!(f, "{name}")
    }
}
#[derive(Debug)]
pub enum Symbol {
    Terminal(Terminal),
    NonTerminal(NonTerminal),
}
#[allow(clippy::upper_case_acronyms)]
#[derive(Debug)]
pub enum Terminal {
    Tb,
    Num(output_dir_actions::Num),
}
#[derive(Debug)]
pub enum NonTerminal {
    A(output_dir_actions::A),
    B1(output_dir_actions::B1),
    B(output_dir_actions::B),
}
pub struct OutputDirParserDefinition {
    actions: [[Action<State, ProdKind>; TERMINAL_COUNT]; STATE_COUNT],
    gotos: [[Option<State>; NONTERMINAL_COUNT]; STATE_COUNT],
}
pub(crate) static PARSER_DEFINITION: OutputDirParserDefinition = OutputDirParserDefinition {
    actions: [
        [Error, Shift(State::TbS1), Error],
        [Error, Reduce(ProdKind::BP1, 1usize), Reduce(ProdKind::BP1, 1usize)],
        [Accept, Error, Error],
        [Error, Shift(State::TbS1), Shift(State::NumS5)],
        [Error, Reduce(ProdKind::B1P2, 1usize), Reduce(ProdKind::B1P2, 1usize)],
        [Reduce(ProdKind::AP1, 2usize), Error, Error],
        [Error, Reduce(ProdKind::B1P1, 2usize), Reduce(ProdKind::B1P1, 2usize)],
    ],
    gotos: [
        [None, None, Some(State::AS2), Some(State::B1S3), Some(State::BS4)],
        [None, None, None, None, None],
        [None, None, None, None, None],
        [None, None, None, None, Some(State::BS6)],
        [None, None, None, None, None],
        [None, None, None, None, None],
        [None, None, None, None, None],
    ],
};
impl ParserDefinition<TokenRecognizer, State, ProdKind, TokenKind, NonTermKind>
for OutputDirParserDefinition {
    fn action(&self, state: State, token: TokenKind) -> Action<State, ProdKind> {
        PARSER_DEFINITION.actions[state as usize][token as usize]
    }
    fn goto(&self, state: State, nonterm: NonTermKind) -> State {
        PARSER_DEFINITION.gotos[state as usize][nonterm as usize].unwrap()
    }
}
type MyContext<'i, I> = LRContext<'i, I, State, TokenKind>;
#[derive(Default)]
pub struct OutputDirParser<'i, I, L: Lexer<Input = I>>(
    LRParser<
        'i,
        MyContext<'i, I>,
        State,
        ProdKind,
        NonTermKind,
        RustemoParserDefinition,
        L,
        B,
        I,
    >,
);
#[allow(dead_code)]
impl<'i, I, L, B> OutputDirParser<'i, I, L, B>
where
    I: Input + ?Sized,
    L: Lexer<'i, MyContext<'i, I>, State, TokenKind, Input = I>,
    B: Builder,
{
    pub fn new() -> Self {
        Self(
            LRParser::new(
                &PARSER_DEFINITION,
                State::default(),
                false,
                false,
                StringLexer::new(true, false, &TOKEN_RECOGNIZERS),
                Some(DefaultBuilder::new(file, input)),
            ),
        );
    }
}
#[allow(dead_code)]
impl<'i, I, L, B> Parser<'i, I, MyContext<'i, I>, L, State, TokenRecognizer>
for OutputDirParser<'i, I, L, B>
where
    I: Input + ?Sized,
    L: Lexer<'i, MyContext<'i, I>, State, TokenKind, Input = I>,
    B: Builder,
{
    type Output = B::Output;
    fn parse(&self, input: &'i I) -> Result<Self::Output> {
        self.0.parse(input)
    }
    fn parse_with_context(
        &self,
        context: &mut MyContext<'i>,
        input: &'i I,
    ) -> Result<Self::Output> {
        self.0.parse_with_context(context, input)
    }
    fn parse_file<'a, F: AsRef<std::path::Path>>(
        &'a mut self,
        file: F,
    ) -> Result<Self::Output>
    where
        'a: 'i,
    {
        self.0.parse_file(file)
    }
}
pub(crate) static RECOGNIZERS: [Option<Lazy<Regex>>; TERMINAL_COUNT] = [
    None,
    None,
    Some(Lazy::new(|| { Regex::new(concat!("^", "\\d+")).unwrap() })),
];
#[allow(dead_code)]
#[derive(Debug)]
pub enum Recognizer {
    Stop,
    StrMatch(&'static str),
    RegexMatch(usize),
}
#[derive(Debug)]
pub struct TokenRecognizer {
    token_kind: TokenKind,
    recognizer: Recognizer,
    finish: bool,
}
impl lexer::TokenRecognizer for TokenRecognizer {
    type TokenKind = TokenKind;
    type Input = str;
    fn recognize<'i>(&self, input: &'i str) -> Option<&'i str> {
        match &self.recognizer {
            Recognizer::StrMatch(s) => {
                logn!("{} {:?} -- ", "\tRecognizing".green(), self.token_kind());
                if input.starts_with(s) {
                    log!("{}", "recognized".bold().green());
                    Some(s)
                } else {
                    log!("{}", "not recognized".red());
                    None
                }
            }
            Recognizer::RegexMatch(r) => {
                logn!("{} {:?} -- ", "\tRecognizing".green(), self.token_kind());
                let match_str = RECOGNIZERS[*r].as_ref().unwrap().find(input);
                match match_str {
                    Some(x) => {
                        let x_str = x.as_str();
                        log!("{} '{}'", "recognized".bold().green(), x_str);
                        Some(x_str)
                    }
                    None => {
                        log!("{}", "not recognized".red());
                        None
                    }
                }
            }
            Recognizer::Stop => {
                logn!("{} STOP -- ", "\tRecognizing".green());
                if input.is_empty() {
                    log!("{}", "recognized".bold().green());
                    Some("")
                } else {
                    log!("{}", "not recognized".red());
                    None
                }
            }
        }
    }
    #[inline]
    fn token_kind(&self) -> TokenKind {
        self.token_kind
    }
    #[inline]
    fn finish(&self) -> bool {
        self.finish
    }
}
impl PartialEq for TokenRecognizer {
    fn eq(&self, other: &Self) -> bool {
        self.token_kind == other.token_kind
    }
}
impl Eq for TokenRecognizer {}
impl Hash for TokenRecognizer {
    fn hash<H: Hasher>(&self, state: &mut H) {
        self.token_kind.hash(state);
    }
}
pub(crate) static TOKEN_RECOGNIZERS: [[Option<
    TokenRecognizer,
>; MAX_RECOGNIZERS]; STATE_COUNT] = [
    [
        [
            Some(TokenRecognizer {
                token_kind: TokenKind::Tb,
                recognizer: Recognizer::StrMatch("b"),
                finish: true,
            }),
            None,
        ],
        [
            Some(TokenRecognizer {
                token_kind: TokenKind::Tb,
                recognizer: Recognizer::StrMatch("b"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Num,
                recognizer: Recognizer::RegexMatch(2usize),
                finish: true,
            }),
        ],
        [
            Some(TokenRecognizer {
                token_kind: TokenKind::STOP,
                recognizer: Recognizer::Stop,
                finish: true,
            }),
            None,
        ],
        [
            Some(TokenRecognizer {
                token_kind: TokenKind::Tb,
                recognizer: Recognizer::StrMatch("b"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Num,
                recognizer: Recognizer::RegexMatch(2usize),
                finish: true,
            }),
        ],
        [
            Some(TokenRecognizer {
                token_kind: TokenKind::Tb,
                recognizer: Recognizer::StrMatch("b"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Num,
                recognizer: Recognizer::RegexMatch(2usize),
                finish: true,
            }),
        ],
        [
            Some(TokenRecognizer {
                token_kind: TokenKind::STOP,
                recognizer: Recognizer::Stop,
                finish: true,
            }),
            None,
        ],
        [
            Some(TokenRecognizer {
                token_kind: TokenKind::Tb,
                recognizer: Recognizer::StrMatch("b"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Num,
                recognizer: Recognizer::RegexMatch(2usize),
                finish: true,
            }),
        ],
    ],
];
pub struct DefaultBuilder {
    res_stack: Vec<Symbol>,
}
impl DefaultBuilder {
    fn new() -> Self {
        Self { res_stack: vec![] }
    }
}
impl Builder for DefaultBuilder {
    type Output = output_dir_actions::A;
    fn get_result(&mut self) -> Self::Output {
        match self.res_stack.pop().unwrap() {
            Symbol::NonTerminal(NonTerminal::A(r)) => r,
            _ => panic!("Invalid result on the parse stack!"),
        }
    }
}
impl<'i, I: Input + ?Sized> LRBuilder<'i, I, ProdKind, TokenKind> for DefaultBuilder {
    #![allow(unused_variables)]
    fn shift_action(
        &mut self,
        context: &mut MyContext<'i, I>,
        token: Token<'i, I, TokenKind>,
    ) {
        let val = match token.kind {
            TokenKind::STOP => panic!("Cannot shift STOP token!"),
            TokenKind::Tb => Terminal::Tb,
            TokenKind::Num => Terminal::Num(output_dir_actions::num(context, token)),
        };
        self.res_stack.push(Symbol::Terminal(val));
    }
    fn reduce_action(
        &mut self,
        context: &mut MyContext<'i, I>,
        prod: ProdKind,
        _prod_len: usize,
    ) {
        let prod = match prod {
            ProdKind::AP1 => {
                let mut i = self
                    .res_stack
                    .split_off(self.res_stack.len() - 2usize)
                    .into_iter();
                match (i.next().unwrap(), i.next().unwrap()) {
                    (
                        Symbol::NonTerminal(NonTerminal::B1(p0)),
                        Symbol::Terminal(Terminal::Num(p1)),
                    ) => NonTerminal::A(output_dir_actions::a_c1(context, p0, p1)),
                    _ => panic!("Invalid symbol parse stack data."),
                }
            }
            ProdKind::B1P1 => {
                let mut i = self
                    .res_stack
                    .split_off(self.res_stack.len() - 2usize)
                    .into_iter();
                match (i.next().unwrap(), i.next().unwrap()) {
                    (
                        Symbol::NonTerminal(NonTerminal::B1(p0)),
                        Symbol::NonTerminal(NonTerminal::B(p1)),
                    ) => NonTerminal::B1(output_dir_actions::b1_c1(context, p0, p1)),
                    _ => panic!("Invalid symbol parse stack data."),
                }
            }
            ProdKind::B1P2 => {
                let mut i = self
                    .res_stack
                    .split_off(self.res_stack.len() - 1usize)
                    .into_iter();
                match i.next().unwrap() {
                    Symbol::NonTerminal(NonTerminal::B(p0)) => {
                        NonTerminal::B1(output_dir_actions::b1_b(context, p0))
                    }
                    _ => panic!("Invalid symbol parse stack data."),
                }
            }
            ProdKind::BP1 => {
                let _ = self
                    .res_stack
                    .split_off(self.res_stack.len() - 1usize)
                    .into_iter();
                NonTerminal::B(output_dir_actions::b_tb(context))
            }
        };
        self.res_stack.push(Symbol::NonTerminal(prod));
    }
}
