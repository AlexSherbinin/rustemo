/// Generated by rustemo. Do not edit manually!
use regex::Regex;
use std::fmt::Debug;
use rustemo::lexer::{self, Token, AsStr};
use rustemo::parser::Parser;
use rustemo::builder::Builder;
use rustemo::Result;
use rustemo::lr::lexer::{
    StringRecognizer, LRStringLexer, LexerDefinition, RecognizerIterator,
};
use rustemo::lr::builder::LRBuilder;
use rustemo::lr::parser::{LRParser, ParserDefinition};
use rustemo::lr::parser::Action::{self, Shift, Reduce, Accept, Error};
use rustemo::index::{StateIndex, TermIndex, NonTermIndex, ProdIndex};
use rustemo::debug::{log, logn};
const TERMINAL_COUNT: usize = 6usize;
const NONTERMINAL_COUNT: usize = 3usize;
const STATE_COUNT: usize = 11usize;
#[allow(dead_code)]
const MAX_ACTIONS: usize = 5usize;
use once_cell::sync::Lazy;
use super::calculator_actions;
pub type Input = str;
pub type Context<'i> = lexer::Context<'i, Input, StateIndex>;
#[allow(clippy::upper_case_acronyms)]
#[derive(Debug, Default, Clone, Copy)]
pub enum TokenKind {
    #[default]
    STOP,
    Number,
    Plus,
    Minus,
    Mul,
    Div,
}
impl AsStr for TokenKind {
    #[allow(dead_code)]
    fn as_str(&self) -> &'static str {
        match self {
            TokenKind::STOP => "STOP",
            TokenKind::Number => "Number",
            TokenKind::Plus => "Plus",
            TokenKind::Minus => "Minus",
            TokenKind::Mul => "Mul",
            TokenKind::Div => "Div",
        }
    }
}
impl From<TermIndex> for TokenKind {
    fn from(term_index: TermIndex) -> Self {
        match term_index.0 {
            0usize => TokenKind::STOP,
            1usize => TokenKind::Number,
            2usize => TokenKind::Plus,
            3usize => TokenKind::Minus,
            4usize => TokenKind::Mul,
            5usize => TokenKind::Div,
            _ => unreachable!(),
        }
    }
}
impl From<TokenKind> for TermIndex {
    fn from(token_kind: TokenKind) -> Self {
        match token_kind {
            TokenKind::STOP => TermIndex(0usize),
            TokenKind::Number => TermIndex(1usize),
            TokenKind::Plus => TermIndex(2usize),
            TokenKind::Minus => TermIndex(3usize),
            TokenKind::Mul => TermIndex(4usize),
            TokenKind::Div => TermIndex(5usize),
        }
    }
}
#[allow(clippy::enum_variant_names)]
#[derive(Clone, Copy)]
pub enum ProdKind {
    EAdd,
    ESub,
    EMul,
    EDiv,
    EP5,
}
impl AsStr for ProdKind {
    #[allow(dead_code)]
    fn as_str(&self) -> &'static str {
        match self {
            ProdKind::EAdd => "EAdd",
            ProdKind::ESub => "ESub",
            ProdKind::EMul => "EMul",
            ProdKind::EDiv => "EDiv",
            ProdKind::EP5 => "EP5",
        }
    }
}
impl std::fmt::Display for ProdKind {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let name = match self {
            ProdKind::EAdd => "E: E Plus E",
            ProdKind::ESub => "E: E Minus E",
            ProdKind::EMul => "E: E Mul E",
            ProdKind::EDiv => "E: E Div E",
            ProdKind::EP5 => "E: Number",
        };
        write!(f, "{}", name)
    }
}
impl From<ProdIndex> for ProdKind {
    fn from(prod_index: ProdIndex) -> Self {
        match prod_index.0 {
            1usize => ProdKind::EAdd,
            2usize => ProdKind::ESub,
            3usize => ProdKind::EMul,
            4usize => ProdKind::EDiv,
            5usize => ProdKind::EP5,
            _ => unreachable!(),
        }
    }
}
#[derive(Debug)]
pub enum Symbol {
    Terminal(Terminal),
    NonTerminal(NonTerminal),
}
#[allow(clippy::upper_case_acronyms)]
#[derive(Debug)]
pub enum Terminal {
    Number(calculator_actions::Number),
    Plus,
    Minus,
    Mul,
    Div,
}
#[derive(Debug)]
pub enum NonTerminal {
    E(calculator_actions::E),
}
pub struct CalculatorParserDefinition {
    actions: [[Action; TERMINAL_COUNT]; STATE_COUNT],
    gotos: [[Option<StateIndex>; NONTERMINAL_COUNT]; STATE_COUNT],
}
pub(crate) static PARSER_DEFINITION: CalculatorParserDefinition = CalculatorParserDefinition {
    actions: [
        [Error, Shift(StateIndex(1usize)), Error, Error, Error, Error],
        [
            Reduce(ProdIndex(5usize), 1usize, NonTermIndex(2usize)),
            Error,
            Reduce(ProdIndex(5usize), 1usize, NonTermIndex(2usize)),
            Reduce(ProdIndex(5usize), 1usize, NonTermIndex(2usize)),
            Reduce(ProdIndex(5usize), 1usize, NonTermIndex(2usize)),
            Reduce(ProdIndex(5usize), 1usize, NonTermIndex(2usize)),
        ],
        [
            Accept,
            Error,
            Shift(StateIndex(3usize)),
            Shift(StateIndex(4usize)),
            Shift(StateIndex(5usize)),
            Shift(StateIndex(6usize)),
        ],
        [Error, Shift(StateIndex(1usize)), Error, Error, Error, Error],
        [Error, Shift(StateIndex(1usize)), Error, Error, Error, Error],
        [Error, Shift(StateIndex(1usize)), Error, Error, Error, Error],
        [Error, Shift(StateIndex(1usize)), Error, Error, Error, Error],
        [
            Reduce(ProdIndex(1usize), 3usize, NonTermIndex(2usize)),
            Error,
            Reduce(ProdIndex(1usize), 3usize, NonTermIndex(2usize)),
            Reduce(ProdIndex(1usize), 3usize, NonTermIndex(2usize)),
            Shift(StateIndex(5usize)),
            Shift(StateIndex(6usize)),
        ],
        [
            Reduce(ProdIndex(2usize), 3usize, NonTermIndex(2usize)),
            Error,
            Reduce(ProdIndex(2usize), 3usize, NonTermIndex(2usize)),
            Reduce(ProdIndex(2usize), 3usize, NonTermIndex(2usize)),
            Shift(StateIndex(5usize)),
            Shift(StateIndex(6usize)),
        ],
        [
            Reduce(ProdIndex(3usize), 3usize, NonTermIndex(2usize)),
            Error,
            Reduce(ProdIndex(3usize), 3usize, NonTermIndex(2usize)),
            Reduce(ProdIndex(3usize), 3usize, NonTermIndex(2usize)),
            Reduce(ProdIndex(3usize), 3usize, NonTermIndex(2usize)),
            Reduce(ProdIndex(3usize), 3usize, NonTermIndex(2usize)),
        ],
        [
            Reduce(ProdIndex(4usize), 3usize, NonTermIndex(2usize)),
            Error,
            Reduce(ProdIndex(4usize), 3usize, NonTermIndex(2usize)),
            Reduce(ProdIndex(4usize), 3usize, NonTermIndex(2usize)),
            Reduce(ProdIndex(4usize), 3usize, NonTermIndex(2usize)),
            Reduce(ProdIndex(4usize), 3usize, NonTermIndex(2usize)),
        ],
    ],
    gotos: [
        [None, None, Some(StateIndex(2usize))],
        [None, None, None],
        [None, None, None],
        [None, None, Some(StateIndex(7usize))],
        [None, None, Some(StateIndex(8usize))],
        [None, None, Some(StateIndex(9usize))],
        [None, None, Some(StateIndex(10usize))],
        [None, None, None],
        [None, None, None],
        [None, None, None],
        [None, None, None],
    ],
};
impl ParserDefinition for CalculatorParserDefinition {
    fn action(&self, state_index: StateIndex, term_index: TermIndex) -> Action {
        PARSER_DEFINITION.actions[state_index.0][term_index.0]
    }
    fn goto(&self, state_index: StateIndex, nonterm_index: NonTermIndex) -> StateIndex {
        PARSER_DEFINITION.gotos[state_index.0][nonterm_index.0].unwrap()
    }
}
#[derive(Default)]
pub struct CalculatorParser {
    content: Option<<Input as ToOwned>::Owned>,
}
#[allow(dead_code)]
impl<'i> CalculatorParser {
    pub fn new() -> Self {
        Self { content: None }
    }
    #[allow(clippy::needless_lifetimes)]
    pub fn parse_file<P: AsRef<std::path::Path>>(
        &'i mut self,
        file: P,
    ) -> Result<<DefaultBuilder as Builder>::Output> {
        self.content = Some(<Input as rustemo::lexer::Input>::read_file(&file)?);
        let mut context = Context::new(
            file.as_ref().to_string_lossy().to_string(),
            self.content.as_ref().unwrap(),
        );
        self.inner_parse(&mut context)
    }
    #[allow(clippy::needless_lifetimes)]
    pub fn parse(
        &self,
        input: &'i Input,
    ) -> Result<<DefaultBuilder as Builder>::Output> {
        let mut context = Context::new("<str>".to_string(), input);
        self.inner_parse(&mut context)
    }
    #[allow(clippy::needless_lifetimes)]
    fn inner_parse(
        &self,
        context: &mut Context<'i>,
    ) -> Result<<DefaultBuilder as Builder>::Output> {
        let local_lexer = LRStringLexer::new(&LEXER_DEFINITION, false, true);
        let lexer = &local_lexer;
        let mut local_builder = DefaultBuilder::new();
        let builder = &mut local_builder;
        let mut parser = LRParser::new(&PARSER_DEFINITION, StateIndex(0));
        parser.parse(context, lexer, builder)
    }
}
pub(crate) static RECOGNIZERS: [Option<Lazy<Regex>>; TERMINAL_COUNT] = [
    None,
    Some(Lazy::new(|| { Regex::new(concat!("^", "\\d+(\\.\\d+)?")).unwrap() })),
    None,
    None,
    None,
    None,
];
#[allow(dead_code)]
pub enum Recognizer {
    Stop,
    StrMatch(&'static str),
    RegexMatch(usize),
}
pub struct TokenRecognizer {
    token_kind: TokenKind,
    recognizer: Recognizer,
    finish: bool,
}
impl StringRecognizer<TokenKind> for TokenRecognizer {
    fn recognize<'i>(&self, input: &'i str) -> Option<&'i str> {
        match &self.recognizer {
            Recognizer::StrMatch(s) => {
                logn!("Recognizing <{:?}> -- ", self.token_kind());
                if input.starts_with(s) {
                    log!("recognized");
                    Some(s)
                } else {
                    log!("not recognized");
                    None
                }
            }
            Recognizer::RegexMatch(r) => {
                logn!("Recognizing <{:?}> -- ", self.token_kind());
                let match_str = RECOGNIZERS[*r].as_ref().unwrap().find(input);
                match match_str {
                    Some(x) => {
                        let x_str = x.as_str();
                        log!("recognized <{}>", x_str);
                        Some(x_str)
                    }
                    None => {
                        log!("not recognized");
                        None
                    }
                }
            }
            Recognizer::Stop => {
                logn!("Recognizing <STOP> -- ");
                if input.is_empty() {
                    log!("recognized");
                    Some("")
                } else {
                    log!("not recognized");
                    None
                }
            }
        }
    }
    #[inline]
    fn token_kind(&self) -> TokenKind {
        self.token_kind
    }
    #[inline]
    fn finish(&self) -> bool {
        self.finish
    }
}
pub struct DefaultLexerDefinition {
    token_rec_for_state: [[Option<TokenRecognizer>; MAX_ACTIONS]; STATE_COUNT],
}
#[allow(clippy::single_char_pattern)]
pub(crate) static LEXER_DEFINITION: DefaultLexerDefinition = DefaultLexerDefinition {
    token_rec_for_state: [
        [
            Some(TokenRecognizer {
                token_kind: TokenKind::Number,
                recognizer: Recognizer::RegexMatch(1usize),
                finish: true,
            }),
            None,
            None,
            None,
            None,
        ],
        [
            Some(TokenRecognizer {
                token_kind: TokenKind::STOP,
                recognizer: Recognizer::Stop,
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Plus,
                recognizer: Recognizer::StrMatch("+"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Minus,
                recognizer: Recognizer::StrMatch("-"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Mul,
                recognizer: Recognizer::StrMatch("*"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Div,
                recognizer: Recognizer::StrMatch("/"),
                finish: true,
            }),
        ],
        [
            Some(TokenRecognizer {
                token_kind: TokenKind::STOP,
                recognizer: Recognizer::Stop,
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Plus,
                recognizer: Recognizer::StrMatch("+"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Minus,
                recognizer: Recognizer::StrMatch("-"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Mul,
                recognizer: Recognizer::StrMatch("*"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Div,
                recognizer: Recognizer::StrMatch("/"),
                finish: true,
            }),
        ],
        [
            Some(TokenRecognizer {
                token_kind: TokenKind::Number,
                recognizer: Recognizer::RegexMatch(1usize),
                finish: true,
            }),
            None,
            None,
            None,
            None,
        ],
        [
            Some(TokenRecognizer {
                token_kind: TokenKind::Number,
                recognizer: Recognizer::RegexMatch(1usize),
                finish: true,
            }),
            None,
            None,
            None,
            None,
        ],
        [
            Some(TokenRecognizer {
                token_kind: TokenKind::Number,
                recognizer: Recognizer::RegexMatch(1usize),
                finish: true,
            }),
            None,
            None,
            None,
            None,
        ],
        [
            Some(TokenRecognizer {
                token_kind: TokenKind::Number,
                recognizer: Recognizer::RegexMatch(1usize),
                finish: true,
            }),
            None,
            None,
            None,
            None,
        ],
        [
            Some(TokenRecognizer {
                token_kind: TokenKind::STOP,
                recognizer: Recognizer::Stop,
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Plus,
                recognizer: Recognizer::StrMatch("+"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Minus,
                recognizer: Recognizer::StrMatch("-"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Mul,
                recognizer: Recognizer::StrMatch("*"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Div,
                recognizer: Recognizer::StrMatch("/"),
                finish: true,
            }),
        ],
        [
            Some(TokenRecognizer {
                token_kind: TokenKind::STOP,
                recognizer: Recognizer::Stop,
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Plus,
                recognizer: Recognizer::StrMatch("+"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Minus,
                recognizer: Recognizer::StrMatch("-"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Mul,
                recognizer: Recognizer::StrMatch("*"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Div,
                recognizer: Recognizer::StrMatch("/"),
                finish: true,
            }),
        ],
        [
            Some(TokenRecognizer {
                token_kind: TokenKind::STOP,
                recognizer: Recognizer::Stop,
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Plus,
                recognizer: Recognizer::StrMatch("+"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Minus,
                recognizer: Recognizer::StrMatch("-"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Mul,
                recognizer: Recognizer::StrMatch("*"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Div,
                recognizer: Recognizer::StrMatch("/"),
                finish: true,
            }),
        ],
        [
            Some(TokenRecognizer {
                token_kind: TokenKind::STOP,
                recognizer: Recognizer::Stop,
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Plus,
                recognizer: Recognizer::StrMatch("+"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Minus,
                recognizer: Recognizer::StrMatch("-"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Mul,
                recognizer: Recognizer::StrMatch("*"),
                finish: true,
            }),
            Some(TokenRecognizer {
                token_kind: TokenKind::Div,
                recognizer: Recognizer::StrMatch("/"),
                finish: true,
            }),
        ],
    ],
};
impl LexerDefinition for DefaultLexerDefinition {
    type TokenRecognizer = TokenRecognizer;
    fn recognizers(
        &self,
        state_index: StateIndex,
    ) -> RecognizerIterator<Self::TokenRecognizer> {
        RecognizerIterator {
            token_rec_for_state: &LEXER_DEFINITION
                .token_rec_for_state[state_index.0][..],
            index: 0,
        }
    }
}
pub struct DefaultBuilder {
    res_stack: Vec<Symbol>,
}
impl Builder for DefaultBuilder {
    type Output = calculator_actions::E;
    fn new() -> Self {
        Self { res_stack: vec![] }
    }
    fn get_result(&mut self) -> Self::Output {
        match self.res_stack.pop().unwrap() {
            Symbol::NonTerminal(NonTerminal::E(r)) => r,
            _ => panic!("Invalid result on the parse stack!"),
        }
    }
}
impl<'i> LRBuilder<'i, Input, TokenKind> for DefaultBuilder {
    #![allow(unused_variables)]
    fn shift_action(
        &mut self,
        context: &mut Context<'i>,
        token: Token<'i, Input, TokenKind>,
    ) {
        let val = match token.kind {
            TokenKind::STOP => panic!("Cannot shift STOP token!"),
            TokenKind::Number => {
                Terminal::Number(calculator_actions::number(context, token))
            }
            TokenKind::Plus => Terminal::Plus,
            TokenKind::Minus => Terminal::Minus,
            TokenKind::Mul => Terminal::Mul,
            TokenKind::Div => Terminal::Div,
        };
        self.res_stack.push(Symbol::Terminal(val));
    }
    fn reduce_action(
        &mut self,
        context: &mut Context<'i>,
        prod_idx: ProdIndex,
        _prod_len: usize,
    ) {
        let prod = match ProdKind::from(prod_idx) {
            ProdKind::EAdd => {
                let mut i = self
                    .res_stack
                    .split_off(self.res_stack.len() - 3usize)
                    .into_iter();
                match (i.next().unwrap(), i.next().unwrap(), i.next().unwrap()) {
                    (
                        Symbol::NonTerminal(NonTerminal::E(p0)),
                        _,
                        Symbol::NonTerminal(NonTerminal::E(p1)),
                    ) => NonTerminal::E(calculator_actions::e_add(context, p0, p1)),
                    _ => panic!("Invalid symbol parse stack data."),
                }
            }
            ProdKind::ESub => {
                let mut i = self
                    .res_stack
                    .split_off(self.res_stack.len() - 3usize)
                    .into_iter();
                match (i.next().unwrap(), i.next().unwrap(), i.next().unwrap()) {
                    (
                        Symbol::NonTerminal(NonTerminal::E(p0)),
                        _,
                        Symbol::NonTerminal(NonTerminal::E(p1)),
                    ) => NonTerminal::E(calculator_actions::e_sub(context, p0, p1)),
                    _ => panic!("Invalid symbol parse stack data."),
                }
            }
            ProdKind::EMul => {
                let mut i = self
                    .res_stack
                    .split_off(self.res_stack.len() - 3usize)
                    .into_iter();
                match (i.next().unwrap(), i.next().unwrap(), i.next().unwrap()) {
                    (
                        Symbol::NonTerminal(NonTerminal::E(p0)),
                        _,
                        Symbol::NonTerminal(NonTerminal::E(p1)),
                    ) => NonTerminal::E(calculator_actions::e_mul(context, p0, p1)),
                    _ => panic!("Invalid symbol parse stack data."),
                }
            }
            ProdKind::EDiv => {
                let mut i = self
                    .res_stack
                    .split_off(self.res_stack.len() - 3usize)
                    .into_iter();
                match (i.next().unwrap(), i.next().unwrap(), i.next().unwrap()) {
                    (
                        Symbol::NonTerminal(NonTerminal::E(p0)),
                        _,
                        Symbol::NonTerminal(NonTerminal::E(p1)),
                    ) => NonTerminal::E(calculator_actions::e_div(context, p0, p1)),
                    _ => panic!("Invalid symbol parse stack data."),
                }
            }
            ProdKind::EP5 => {
                let mut i = self
                    .res_stack
                    .split_off(self.res_stack.len() - 1usize)
                    .into_iter();
                match i.next().unwrap() {
                    Symbol::Terminal(Terminal::Number(p0)) => {
                        NonTerminal::E(calculator_actions::e_number(context, p0))
                    }
                    _ => panic!("Invalid symbol parse stack data."),
                }
            }
        };
        self.res_stack.push(Symbol::NonTerminal(prod));
    }
}
